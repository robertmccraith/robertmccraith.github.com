

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Publications - Robert McCraith</title>







<meta property="og:locale" content="en-GB">
<meta property="og:site_name" content="Robert McCraith">
<meta property="og:title" content="Publications">


  <link rel="canonical" href="http://localhost:4000/publications/">
  <meta property="og:url" content="http://localhost:4000/publications/">







  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Robert McCraith",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Robert McCraith Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>


<!-- Support for MatJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Robert McCraith</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.png" class="author__avatar" alt="Robert McCraith">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Robert McCraith</h3>
    
    <p class="author__bio">Research Scientist specialising in computer vision and machine learning</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
        <li><i class="fa-solid fa-location-dot icon-pad-right" aria-hidden="true"></i>Oxford</li>
      
      
      
      
        <li><a href="mailto:mccraithrobert@gmail.com"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
      
        <li><a href="https://arxiv.org/search/cs?searchtype=author&query=McCraith,+R"><i class="ai ai-arxiv ai-fw icon-pad-right"></i>arXiv</a></li>
            
      
        <li><a href="https://scholar.google.com/citations?user=tzTjh0wAAAAJ"><i class="ai ai-google-scholar icon-pad-right"></i>Google Scholar</a></li>
      
      
      
                              
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
        <li><a href="https://github.com/robertmccraith"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>Github</a></li>
      
            
            

      <!-- Font Awesome icons / Social media -->
      
      
            
      
                  
                  
      
            
            
      
        <li><a href="https://www.linkedin.com/in/robert-mccraith"><i class="fab fa-fw fa-linkedin icon-pad-right" aria-hidden="true"></i>LinkedIn</a></li>
            
      
            
                  
            
      
            
            
      
        <li><a href="https://twitter.com/robmccr"><i class="fab fa-fw fa-x-twitter icon-pad-right" aria-hidden="true"></i>X (formerly Twitter)</a></li>
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 class="page__title">Publications</h1>
    
    
<div class="wordwrap">You can also find my articles on <a href="https://scholar.google.com/citations?user=tzTjh0wAAAAJ">my Google Scholar profile</a>.</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/instruction-tuning-for-large-language-models" rel="permalink">Instruction tuning for large language models: the impact of human-inspired learning strategies
</a>
      
    </h2>

    

    
    <p><i>To be submitted to COLM</i>, 2024 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    

    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2024-10-11-exploring-the-landscape-of-large-language-models-in-medical-question-answering" rel="permalink">Exploring The landscape of Large Language Models In Medical Question Answering: Observations and Open Questions
</a>
      
    </h2>

    

    
    <p><i>Submitted to NEJM AI</i>, 2024 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://arxiv.org/abs/2310.07225 ">here</a></p>
    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2022-12-03-direct-lidar" rel="permalink">Direct LiDAR-based object detector training from automated 2D detections
</a>
      
    </h2>

    

    
    <p><i>NeurIPS Workshop on Machine Learning for Autonomous Driving (ML4AD)</i>, 2022 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>3D Object detection (3DOD) is an important component of many applications, however existing methods rely heavily on datasets of depth and image data which require expensive annotation in 3D thus limiting the ability of a diverse dataset being collected which truly represents the long tail of potential scenes in the wild. In this work we propose to utilise a readily available robust 2D Object Detector and to transfer information about objects from 2D to 3D, allowing us to train a 3D Object Detector without the need for any human annotation in 3D. We demonstrate that our method significantly outperforms previous 3DOD methods supervised by only 2D annotations, and that our method narrows the accuracy gap between methods that use 3D supervision and those that do not.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://ml4ad.github.io/files/papers2022/Direct%20LiDAR-based%20Object%20Detector%20Training%20from%20Automated%202D%20Detections.pdf ">here</a></p>
    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2022-05-23-lifting-2d-object-locations-to-3d-by-discounting-lidar-outliers-across-objects-and-views" rel="permalink">Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views
</a>
      
    </h2>

    

    
    <p><i>ICRA</i>, 2022 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>We present a system for automatic converting of 2D mask object predictions and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the LiDAR point clouds are partial, directly fitting bounding boxes to the point clouds is meaningless. Instead, we suggest that obtaining good results requires sharing information between \emph{all} objects in the dataset jointly, over multiple frames. We then make three improvements to the baseline. First, we address ambiguities in predicting the object rotations via direct optimization in this space while still backpropagating rotation prediction through the model. Second, we explicitly model outliers and task the network with learning their typical patterns, thus better discounting them. Third, we enforce temporal consistency when video data is available. With these contributions, our method significantly outperforms previous work despite the fact that those methods use significantly more complex pipelines, 3D models and additional human-annotated external sources of prior information.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://arxiv.org/abs/2109.07945 ">here</a></p>
    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2021-10-13-calibrating" rel="permalink">Calibrating Self-supervised Monocular Depth Estimation
</a>
      
    </h2>

    

    
    <p><i>NeurIPS Workshop on Machine Learning for Autonomous Driving (ML4AD)</i>, 2021 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>In the recent years, many methods demonstrated the ability of neural networks to learn depth and pose changes in a sequence of images, using only self-supervision as the training signal. Whilst the networks achieve good performance, the often over-looked detail is that due to the inherent ambiguity of monocular vision they predict depth up to an unknown scaling factor. The scaling factor is then typically obtained from the LiDAR ground truth at test time, which severely limits practical applications of these methods. In this paper, we show that incorporating prior information about the camera configuration and the environment, we can remove the scale ambiguity and predict depth directly, still using the self-supervised formulation and not relying on any additional sensors.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://arxiv.org/abs/2004.05821 ">here</a></p>
    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2021-09-16-real-time-monocular-vehicle-velocity-estimation-using-synthetic-data" rel="permalink">Real Time Monocular Vehicle Velocity Estimation using Synthetic Data
</a>
      
    </h2>

    

    
    <p><i>IEEE Intelligent Vehicles</i>, 2021 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Vision is one of the primary sensing modalities in autonomous driving. In this paper we look at the problem of estimating the velocity of road vehicles from a camera mounted on a moving car. Contrary to prior methods that train end-to-end deep networks that estimate the vehiclesâ€™ velocity from the video pixels, we propose a two-step approach where first an off-the-shelf tracker is used to extract vehicle bounding boxes and then a small neural network is used to regress the vehicle velocity from the tracked bounding boxes. Surprisingly, we find that this still achieves state-of-the-art estimation performance with the significant benefit of separating perception from dynamics estimation via a clean, interpretable and verifiable interface which allows us distill the statistics which are crucial for velocity estimation. We show that the latter can be used to easily generate synthetic training data in the space of bounding boxes and use this to improve the performance of our method further.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://arxiv.org/abs/2109.07957 ">here</a></p>
    

  </article>
</div>

<div class=" list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
      <a href="http://localhost:4000/publication/2020-04-13-monocular-depth-estimation-with-self-supervised-instance-adaptation" rel="permalink">Monocular Depth Estimation with Self-supervised Instance Adaptation
</a>
      
    </h2>

    

    
    <p><i>ArXiv</i>, 2020 </p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Recent advances in self-supervised learning have demonstrated that it is possible to learn accurate monocular depth reconstruction from raw video data, without using any 3D ground truth for supervision. However, in robotics applications, multiple views of a scene may or may not be available, depending on the actions of the robot, switching between monocular and multi-view reconstruction. To address this mixed setting, we proposed a new approach that extends any off-the-shelf self-supervised monocular depth reconstruction system to usemore than one image at test time. Our method builds on a standard prior learned to perform monocular reconstruction, but uses self-supervision at test time to further improve the reconstruction accuracy when multiple images are available. When used to update the correct components of the model, this approach is highly-effective. On the standard KITTI bench-mark, our self-supervised method consistently outperforms all the previous methods with an average 25% reduction in absolute error for the three common setups (monocular, stereo and monocular+stereo), and comes very close in accuracy when compared to the fully-supervised state-of-the-art methods.</p>
</p>
    

    
    <p class="wordwrap">Download <a href=" https://arxiv.org/abs/2004.05821 ">here</a></p>
    

  </article>
</div>


  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
    <li><strong>Follow:</strong></li>
    
    
    
    
    <li><a href="http://github.com/robertmccraith"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a>
    </li>
    
    
    <!-- <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Robert McCraith. Powered by <a href="http://jekyllrb.com"
    rel="nofollow">Jekyll</a> &amp; <a
    href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a
    href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

